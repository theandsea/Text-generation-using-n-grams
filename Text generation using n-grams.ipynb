{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09e3e1aa-c899-44e3-8994-9241392143c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class NGramModel(object):\n",
    "    def __init__(self, n: int) -> None:\n",
    "        self.n = n\n",
    "        self.n_grams = dict() # Stores unique n-grams\n",
    "        self.context_count = dict() # Stores count of contexts\n",
    "        self.ngram_count = dict() # Stores count of ngrams: (context, token)\n",
    "        \n",
    "        \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        # Treat punctuation as a separate token.\n",
    "        # Add space before punctuation.\n",
    "        curr=text\n",
    "        punct=\"!\\\"#$%&'()*+,-—./:;<=>?@[\\]^_`{|}~\" #’\n",
    "        for p in punct:\n",
    "            curr=curr.replace(p,' '+p)\n",
    "        # Split using spaces.\n",
    "        tokens=curr.split(' ')\n",
    "        return tokens\n",
    "        \n",
    "\n",
    "    def generate_n_grams(self, tokens: List[str]) -> List[Tuple[List[str], str]]:\n",
    "        \"\"\"\n",
    "        n_grams is a list where each element is\n",
    "        ([n-1 context tokens], token)\n",
    "        \"\"\"\n",
    "        # Prefix the <START> tokens before each sentence\n",
    "        tokens = (self.n-1)*[\"<START>\"] + tokens\n",
    "        n_grams = list()\n",
    "        n=self.n\n",
    "        for i in range(len(tokens)-n+1):\n",
    "            context=tokens[i:i+n-1]\n",
    "            tok=tokens[i+n-1]\n",
    "            n_grams.append((context,tok))\n",
    "        return n_grams\n",
    "\n",
    "    def fit(self, text: str) -> None:\n",
    "        new_n_grams = self.generate_n_grams(self.tokenize(text))\n",
    "        for context, target in new_n_grams:\n",
    "            # Add context to context dict and store count \n",
    "            if tuple(context) in self.context_count:\n",
    "                self.context_count[tuple(context)] += 1.0\n",
    "            else:\n",
    "                self.context_count[tuple(context)] = 1.0\n",
    "\n",
    "            # Save unique n_grams.\n",
    "            if tuple(context) in self.n_grams:        \n",
    "                if target not in self.n_grams[tuple(context)]:\n",
    "                    self.n_grams[tuple(context)].append(target)\n",
    "            else:\n",
    "                self.n_grams[tuple(context)] = [target]\n",
    "\n",
    "            # Store n_gram counts\n",
    "            new_n_gram = (tuple(context), target)\n",
    "            if new_n_gram in self.ngram_count:\n",
    "                self.ngram_count[new_n_gram] += 1.0\n",
    "            else:\n",
    "                self.ngram_count[new_n_gram] = 1.0\n",
    "\n",
    "\n",
    "    def get_prob(self, context: List[str], target: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the probability of each token\n",
    "        associated with the context.\n",
    "        \"\"\"\n",
    "        ctxt_tup=tuple(context)\n",
    "        denominator=self.context_count[ctxt_tup]\n",
    "        numerator=self.ngram_count[(ctxt_tup,target)]\n",
    "        prob=numerator/denominator\n",
    "\n",
    "        return prob\n",
    "\n",
    "    def predict_token(self, context: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Predicts token.\n",
    "        A slight randomness ensures we generate a diverse token\n",
    "        with the same context.\n",
    "        \"\"\"\n",
    "        r = random.random()\n",
    "        # store the probability of each token.\n",
    "        token_probs = dict()\n",
    "\n",
    "        tokens_of_interest = self.n_grams[tuple(context)]\n",
    "        for token in tokens_of_interest:\n",
    "            token_probs[token] = self.get_prob(context, token)\n",
    "\n",
    "        sum = 0.0\n",
    "        for key in sorted(token_probs):\n",
    "            sum += token_probs[key]\n",
    "            # When the probability sum is \n",
    "            # greater than the random number\n",
    "            # we return the current token.\n",
    "            if sum > r:\n",
    "                return key\n",
    "\n",
    "def generate_text(model: NGramModel, n_outs: int) -> str:\n",
    "    \"\"\"\n",
    "    Genertes n_outs words using the trained\n",
    "    ngram model.\n",
    "    \"\"\"\n",
    "    n = model.n\n",
    "    # All sentence are initialized with the <START> token\n",
    "    context_queue = (n-1) * [\"<START>\"]\n",
    "    result = list()\n",
    "\n",
    "    for _ in range(n_outs):\n",
    "        pred_token = model.predict_token(context_queue)\n",
    "        result.append(pred_token)\n",
    "\n",
    "        context_queue.pop(0)\n",
    "        \n",
    "        if pred_token == \".\":\n",
    "            # If sentence done. Start a new sentence.\n",
    "            context_queue = (n-1) * [\"<START>\"]\n",
    "        else:\n",
    "            context_queue.append(pred_token)\n",
    "\n",
    "    return \" \".join(result)\n",
    "\n",
    "\n",
    "def create_ngram_model(model: NGramModel, path: str) -> None:\n",
    "    \"\"\"\n",
    "    fits the ngram model.\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        # Split into sentences.\n",
    "        text = text.split('.')\n",
    "        for sentence in text:\n",
    "            # add back the fullstop\n",
    "            sentence += '.'\n",
    "            model.fit(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26f27589-a649-4f6e-bf71-ee7d68781e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writetext(leng):\n",
    "    start = time.time()\n",
    "    \n",
    "    m = NGramModel(n=leng) # Initialize NGram model\n",
    "\n",
    "    # Fit the ngram model on Shakespeare plays\n",
    "    ROOT=\"./Shakespeare\"\n",
    "    for root, dird, files in os.walk(ROOT):\n",
    "        for text_file in files:\n",
    "            path = os.path.join(root, text_file)\n",
    "            print(\"Loading \", path)\n",
    "            create_ngram_model(m, path)\n",
    "\n",
    "    print (f'Language Model creating time: {time.time() - start}')\n",
    "    \n",
    "    # Generate Text using the trained ngram model.\n",
    "    num_gen_words = 100\n",
    "    \n",
    "    print(f'{\"=\"*50}\\nGenerated text:')\n",
    "    print(\"\\n\")\n",
    "    print(generate_text(m, num_gen_words))\n",
    "    print(f'{\"=\"*50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66e2faba-7148-4091-8098-6044bb39a5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "========================================== n= 2  ==============================================\n",
      "Loading  ./Shakespeare\\Hamlet.txt\n",
      "Loading  ./Shakespeare\\Macbeth.txt\n",
      "Loading  ./Shakespeare\\Othello.txt\n",
      "Loading  ./Shakespeare\\Romeo_and_Juliet.txt\n",
      "Loading  ./Shakespeare\\Tempest.txt\n",
      "Language Model creating time: 0.6798264980316162\n",
      "==================================================\n",
      "Generated text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Enter Ghost . \n",
      "\n",
      "IAGO .  There’s a back to pieces .   That both friend ?\n",
      "\n",
      "CLOWN .    Too much is well , my soule her fair for my old cakes of . \n",
      "\n",
      "OPHELIA . \n",
      "\n",
      "Burthen : Then gave it needs be wrencht with the wall . \n",
      "\n",
      "    Nurse .  [ _Sings .  Oh , that is some other shelter hereabout ,\n",
      "And Princesse can it hard , wormwood on mine own kisses sin . \n",
      "\n",
      "HORATIO .  You speak to the dust ; say to be brain 'd in love\n",
      "==================================================\n",
      "====================================================================================================\n",
      "========================================== n= 3  ==============================================\n",
      "Loading  ./Shakespeare\\Hamlet.txt\n",
      "Loading  ./Shakespeare\\Macbeth.txt\n",
      "Loading  ./Shakespeare\\Othello.txt\n",
      "Loading  ./Shakespeare\\Romeo_and_Juliet.txt\n",
      "Loading  ./Shakespeare\\Tempest.txt\n",
      "Language Model creating time: 0.7434020042419434\n",
      "==================================================\n",
      "Generated text:\n",
      "\n",
      "\n",
      " I tremble at this haste ?\n",
      "  Ross .  I did stay .  I that name , Iago and Gentlemen .  I thinke hee will carry no crotchets . \n",
      "      Or else depart . \n",
      "\n",
      " Enter Lucianus .  —Do not learn of him through me , never tell me ,\n",
      "    Ste .  Hee dy 'de ,\n",
      "As one incapable of her revolt ,\n",
      "For she had been\n",
      "blessed , she 's mine ;\n",
      "I chose her when they ask you\n",
      "what it means , that follows there , they rore\n",
      "\n",
      " \n",
      "==================================================\n",
      "====================================================================================================\n",
      "========================================== n= 4  ==============================================\n",
      "Loading  ./Shakespeare\\Hamlet.txt\n",
      "Loading  ./Shakespeare\\Macbeth.txt\n",
      "Loading  ./Shakespeare\\Othello.txt\n",
      "Loading  ./Shakespeare\\Romeo_and_Juliet.txt\n",
      "Loading  ./Shakespeare\\Tempest.txt\n",
      "Language Model creating time: 0.6222922801971436\n",
      "==================================================\n",
      "Generated text:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "OTHELLO .  Besides , this Duncane\n",
      "Hath borne his Faculties so meeke ; hath bin\n",
      "So cleere in his great Office , that his Issue\n",
      "Should become Kings of Naples ? O reioyce\n",
      "Beyond a common ioy , and set it downe\n",
      "With gold on lasting Pillers : In one voyage\n",
      "Did Claribell her husband finde at Tunis ,\n",
      "And Ferdinand her brother , found a wife ,\n",
      "Where he himselfe was lost : Prospero , his Dukedome\n",
      "In a poore Isle : and all of vs : and the faire soule her selfe\n",
      "Waigh 'd betweene loathnesse , and obedience , at\n",
      "Which end o 'th ' day ?\n",
      "\n",
      "\n",
      "==================================================\n",
      "====================================================================================================\n",
      "========================================== n= 5  ==============================================\n",
      "Loading  ./Shakespeare\\Hamlet.txt\n",
      "Loading  ./Shakespeare\\Macbeth.txt\n",
      "Loading  ./Shakespeare\\Othello.txt\n",
      "Loading  ./Shakespeare\\Romeo_and_Juliet.txt\n",
      "Loading  ./Shakespeare\\Tempest.txt\n",
      "Language Model creating time: 0.6105048656463623\n",
      "==================================================\n",
      "Generated text:\n",
      "\n",
      "\n",
      " Ay , marry , why was he sent into England ?\n",
      "\n",
      "FIRST CLOWN . \n",
      "\n",
      "  Jul .  Worthy Macduffe , and wee\n",
      "Shall take vpon 's what else remaines to do ,\n",
      "According to our order\n",
      "\n",
      "   Sey .  And\n",
      "now remains That we find out the cause of this effect , Or rather say ,\n",
      "the cause of this defect , For this effect defective comes by cause .  Foure legges and two voyces ; a most delicate\n",
      "Monster : his forward voyce now is to speake well of\n",
      "his friend ; his backward voice , is to vtter foule\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    leng=[2,3,4,5]\n",
    "    for n in leng:\n",
    "        print(\"====================================================================================================\")\n",
    "        print(\"========================================== n=\",n,\" ==============================================\")\n",
    "        writetext(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f49b5b1-0bc4-4b42-a653-f7d837f9e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BARNARDO', '.', 'Looks', 'it', 'not', 'like', 'the', 'King', '?', 'Mark', 'it', ',', 'Horatio', '.']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text: str) -> List[str]:\n",
    "    # Treat punctuation as a separate token.\n",
    "    # Add space before punctuation.\n",
    "    punct=\"!\\\"#$%&'()*+,-—./:;<=>?@[\\]^_`{|}~\"\n",
    "    curr=text\n",
    "    for p in punct:\n",
    "        curr=curr.replace(p,' '+p)\n",
    "    # Split using spaces.\n",
    "    return curr.split(' ')\n",
    "\n",
    "print(tokenize(\"BARNARDO. Looks it not like the King? Mark it, Horatio.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f32d4-c49d-45db-b125-f579cb69f5b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
